## 1. 서론
Transformer는 딥러닝 기반 자연어처리(NLP) 분야에서 혁신적인 변화를 가져온 모델로, 2017년 Vaswani et al.의 논문 "Attention Is All You Need"에서 처음 등장했습니다. RNN이나 LSTM과 달리 순차적 처리 구조가 아닌, 병렬 처리 구조를 지원하여 연산 효율성이 높고 긴 문장 처리에 강점을 보입니다. 본 논문에서는 Transformer의 구조를 심도 있게 분석하고, 각 구성 요소들이 어떠한 역할을 하는지를 차근차근 살펴보고자 합니다. 아울러 Transformer가 기존 모델 대비 가지는 이점과 한계점도 논의합니다. 이를 통해 Transformer 아키텍처의 근본 원리와 응용 가능성에 대해 명확히 이해할 수 있습니다.
