## 3. Self-Attention 메커니즘
Self-Attention은 한 문장 내에서 각 단어가 다른 단어와 얼마나 연관되어 있는지 계산하는 메커니즘입니다. 입력 시퀀스 내의 각 단어는 쿼리(Query), 키(Key), 밸류(Value)로 변환됩니다. 각 쿼리는 모든 키와의 유사도를 계산하여 attention score를 만듭니다. 이 score와 각 밸류의 가중합을 구하면 각 단어마다 다른 입력 단어와의 관계가 반영된 새로운 벡터 표현이 형성됩니다. 이는 문맥적 의존성을 효과적으로 학습할 수 있게 하며, 특히 장기적인 의존성 문제(Long-distance dependency)를 획기적으로 해결합니다. 

다음 차트는 Self-Attention의 기본 작동 과정을 요약합니다.

| Step            | Description                          |
|-----------------|--------------------------------------|
| Input           | 단어 임베딩 값을 입력                |
| Q, K, V 계산   | 각 단어별 Query, Key, Value 산출   |
| Score 계산      | Query-Key 내적 후 Softmax           |
| 가중합          | Score와 Value의 가중합 구함        |
| Output          | Self-Attention 벡터 출력            |
