## 4. Position Encoding 기법
Transformer는 RNN과 달리 시퀀스의 순서 정보를 명시적으로 받을 수 없습니다. 그래서 각 단어 임베딩에 순서를 암시하는 Position Encoding을 더해줍니다. Position Encoding은 사인 및 코사인 함수를 이용하거나 학습 가능한 임베딩 형태로 구현됩니다. 이 방식은 모델이 단어나 문장의 상대적, 절대적 위치를 파악할 수 있게 해줍니다. 이를 통해 순서 정보가 모델에 반영되어 문맥 이해도가 향상됩니다. 아래 표는 사인/코사인 기반 Position Encoding의 공식을 요약합니다.

| 계산 방식 | 수식 |
|----------|-------------------------------|
| 짝수  | $PE(pos, 2i) = \sin(\frac{pos}{10000^{2i/d_{model}}})$ |
| 홀수  | $PE(pos, 2i+1) = \cos(\frac{pos}{10000^{2i/d_{model}}})$ |