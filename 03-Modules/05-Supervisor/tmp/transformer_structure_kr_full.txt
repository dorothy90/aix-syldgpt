1. 1. 서론
2. 2. Transformer의 기본 구조
3. 3. Self-Attention 메커니즘
4. 4. Position Encoding 기법
5. 5. 인코더-디코더 구조
6. 6. Multi-Head Attention
7. 7. Residual Connection과 Layer Normalization
8. 8. Feed-Forward Neural Network
9. 9. Transformer의 장점 및 한계
10. 10. 결론
