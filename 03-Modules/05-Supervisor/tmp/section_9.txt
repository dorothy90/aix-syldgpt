## 9. Transformer의 장점 및 한계
Transformer는 병렬 처리와 장기 의존성 학습에 강점을 지닙니다. 특히 Self-Attention 기반 구조는 멀리 떨어진 단어 간 관계도 쉽게 포착할 수 있습니다. 하지만 입력 길이가 길어질수록 연산량이 기하급수적으로 증가하며, 메모리 사용량이 많아지는 한계가 있습니다. 또한, Position Encoding이 단순하여 복잡한 순서 정보 포착에 한계가 존재할 수 있습니다. 현재는 이를 개선하기 위한 다양한 변형 모델이 연구되고 있습니다.