# %%
from pptx import Presentation
from openpyxl import load_workbook
import os
from dotenv import load_dotenv
from pathlib import Path
import base64
from openai import OpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from pymongo import MongoClient
import numpy as np

# %%
# API KEY ì •ë³´ë¡œë“œ
load_dotenv(override=True)
embeddings_model_name = os.getenv("EMBEDDINGS_MODEL_NAME")
vl_model_name = os.getenv("VL_MODEL_NAME")
# í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
text_embedder = OpenAIEmbeddings(
    model=embeddings_model_name,
    openai_api_key=os.getenv("OPENROUTER_API_KEY"),
    openai_api_base=os.getenv("OPENROUTER_BASE_URL"),
)

vision_client = OpenAI(
    api_key=os.getenv("OPENROUTER_API_KEY"),
    base_url=os.getenv("OPENROUTER_BASE_URL"),
)


# ê²€ìƒ‰/ì„ë² ë”©ìš© ê³µí†µ ì •ê·œí™” í•¨ìˆ˜ (ëŒ€/ì†Œë¬¸ì ë¬´ì‹œ)
def normalize_text(text: str) -> str:
    q = str(text).replace("\r\n", "\n").replace("\r", "\n")
    q = "\n".join(line.rstrip() for line in q.split("\n")).strip()
    return q.casefold()


# MongoDB ì—°ê²°
mongo_client = MongoClient("mongodb://localhost:27017/")
db = mongo_client["document_vectorstore"]
collection = db["embeddings"]


# %%
# 1. PPTX ì²˜ë¦¬ í•¨ìˆ˜
def extract_pptx(file_path, output_dir="output_images"):
    """PPTXì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì¶”ì¶œ"""
    os.makedirs(output_dir, exist_ok=True)
    prs = Presentation(file_path)
    slide_data = []

    for i, slide in enumerate(prs.slides):
        slide_text = []
        slide_images = []

        for j, shape in enumerate(slide.shapes):
            if hasattr(shape, "text"):
                slide_text.append(shape.text)
            if shape.shape_type == 13:  # PICTURE
                image = shape.image
                image_bytes = image.blob
                ext = image.ext
                filename = Path(file_path).stem
                image_filename = f"{output_dir}/{filename}_slide_{i}_img_{j}.{ext}"
                with open(image_filename, "wb") as f:
                    f.write(image_bytes)
                slide_images.append(image_filename)

        slide_data.append(
            {
                "text": "\n".join(slide_text),
                "images": slide_images,
                "page_number": i + 1,
            }
        )

    return slide_data


def describe_image(image_path):
    """ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±"""
    with open(image_path, "rb") as img_file:
        b64_img = base64.b64encode(img_file.read()).decode()
        response = vision_client.chat.completions.create(
            model=vl_model_name,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Describe this image in detail."},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"},
                        },
                    ],
                }
            ],
            max_tokens=512,
        )
    return response.choices[0].message.content


def process_pptx(file_path, output_dir="output_images"):
    """PPTXë¥¼ Document ê°ì²´ë¡œ ë³€í™˜"""
    slide_data = extract_pptx(file_path, output_dir)

    # ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
    for slide in slide_data:
        slide["image_descriptions"] = [describe_image(img) for img in slide["images"]]

    # Document ê°ì²´ ìƒì„±
    documents = []
    for slide in slide_data:
        content_parts = []

        if slide.get("text", "").strip():
            content_parts.append(f"[ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸]\n{slide['text']}")

        if slide.get("image_descriptions"):
            for img_idx, img_desc in enumerate(slide["image_descriptions"]):
                content_parts.append(f"\n[ì´ë¯¸ì§€ {img_idx + 1} ì„¤ëª…]\n{img_desc}")

        page_content = "\n".join(content_parts)

        doc = Document(
            page_content=page_content,
            metadata={
                "source": file_path,
                "doc_type": "pptx",
                "page_number": slide["page_number"],
                "slide_text": slide.get("text", ""),
                "image_count": len(slide.get("images", [])),
                "image_paths": slide.get("images", []),
            },
        )
        documents.append(doc)

    return documents


# %%
# 2. Excel ì²˜ë¦¬ í•¨ìˆ˜
def process_excel(file_path):
    """Excel íŒŒì¼ì„ Document ê°ì²´ë¡œ ë³€í™˜ (í–‰ë³„ ê²€ìƒ‰ ìµœì í™”)"""
    import pandas as pd
    from langchain_community.document_loaders import DataFrameLoader

    # Excel íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ë¥¼ ì½ê¸°
    excel_file = pd.ExcelFile(file_path)
    documents = []

    for sheet_name in excel_file.sheet_names:
        # ê° ì‹œíŠ¸ë¥¼ DataFrameìœ¼ë¡œ ì½ê¸°
        df = pd.read_excel(file_path, sheet_name=sheet_name)

        # DataFrameì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì²˜ë¦¬
        if not df.empty:
            # ëª¨ë“  ì»¬ëŸ¼ì„ ì½ê¸° ì‰½ê²Œ ê²°í•© (ë²¡í„° ê²€ìƒ‰ìš©)
            df["_combined_content"] = df.apply(
                lambda row: ", ".join(
                    [f"{col}: {val}" for col, val in row.items() if pd.notna(val)]
                ),
                axis=1,
            )

            # DataFrameLoaderë¡œ ë¬¸ì„œ ìƒì„±
            loader = DataFrameLoader(df, page_content_column="_combined_content")
            sheet_docs = loader.load()

            # ê° í–‰ì„ ìˆœíšŒí•˜ë©´ì„œ ë©”íƒ€ë°ì´í„° ë³´ê°•
            for idx, doc in enumerate(sheet_docs):
                row_data = df.iloc[idx]

                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                doc.metadata.update(
                    {
                        "source": file_path,
                        "doc_type": "excel",
                        "sheet_name": sheet_name,
                        "row_number": idx + 2,  # Excel í–‰ ë²ˆí˜¸ (í—¤ë” í¬í•¨)
                    }
                )

                # ì›ë³¸ DataFrameì˜ ê° ì»¬ëŸ¼ì„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                # (ìˆ«ì í•„í„°ë§ì„ ìœ„í•´)
                for col in df.columns:
                    if col != "_combined_content":
                        value = row_data[col]
                        # NaNì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                        if pd.notna(value):
                            # ìˆ«ì íƒ€ì…ì€ ê·¸ëŒ€ë¡œ, ë‚˜ë¨¸ì§€ëŠ” ë¬¸ìì—´ë¡œ
                            if isinstance(value, (int, float)):
                                doc.metadata[col] = float(value)
                            else:
                                doc.metadata[col] = str(value)

            documents.extend(sheet_docs)

    return documents


# 3. ì¼ë°˜ í…ìŠ¤íŠ¸/PDF/Word ì²˜ë¦¬ í•¨ìˆ˜
def process_text_document(file_path):
    """TXT, PDF, DOCX ë“±ì„ Document ê°ì²´ë¡œ ë³€í™˜"""
    from langchain_community.document_loaders import (
        TextLoader,
        PyPDFLoader,
        Docx2txtLoader,
    )
    from langchain_text_splitters import RecursiveCharacterTextSplitter

    file_ext = Path(file_path).suffix.lower()

    # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ë¡œë” ì„ íƒ
    if file_ext == ".txt":
        loader = TextLoader(file_path, encoding="utf-8")
        doc_type = "text"
    elif file_ext == ".pdf":
        loader = PyPDFLoader(file_path)
        doc_type = "pdf"
    elif file_ext in [".docx", ".doc"]:
        loader = Docx2txtLoader(file_path)
        doc_type = "word"
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")

    # ë¬¸ì„œ ë¡œë“œ
    documents = loader.load()

    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    for doc in documents:
        doc.metadata["doc_type"] = doc_type

    # í…ìŠ¤íŠ¸ ë¶„í•  (ê¸´ ë¬¸ì„œì˜ ê²½ìš°)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=250,
        chunk_overlap=50,
        separators=[r"\n---\n", "\n\n", "\n", " ", ""],
        is_separator_regex=True,
    )
    split_documents = text_splitter.split_documents(documents)

    return split_documents


# 4. Goodocs ì²˜ë¦¬ í•¨ìˆ˜ (ìš©ì–´ì‚¬ì „íŠ¹í™”)
def process_goodocs(file_path):
    """Excel íŒŒì¼ì„ Document ê°ì²´ë¡œ ë³€í™˜ (ìš©ì–´ ì‚¬ì „ ìµœì í™”)"""
    import pandas as pd
    from langchain_community.document_loaders import DataFrameLoader

    # Excel íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ë¥¼ ì½ê¸°
    excel_file = pd.ExcelFile(file_path)
    documents = []

    for sheet_name in excel_file.sheet_names:
        # ê° ì‹œíŠ¸ë¥¼ DataFrameìœ¼ë¡œ ì½ê¸°
        df = pd.read_excel(file_path, sheet_name=sheet_name)

        # DataFrameì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì²˜ë¦¬
        if not df.empty:
            # ìš©ì–´-ì„¤ëª… í˜•íƒœì— ìµœì í™”ëœ content ìƒì„±
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼(ìš©ì–´)ê³¼ ë‘ ë²ˆì§¸ ì»¬ëŸ¼(ì„¤ëª…)ì„ ëª…í™•í•˜ê²Œ êµ¬ì¡°í™”
            first_col = df.columns[0]  # ìš©ì–´ ì»¬ëŸ¼
            second_col = (
                df.columns[1] if len(df.columns) > 1 else first_col
            )  # ì„¤ëª… ì»¬ëŸ¼

            df["_combined_content"] = df.apply(
                lambda row: (
                    f"{row[first_col]}: {row[second_col]}"
                    if pd.notna(row[second_col])
                    else str(row[first_col])
                ),
                axis=1,
            )

            # DataFrameLoaderë¡œ ë¬¸ì„œ ìƒì„±
            loader = DataFrameLoader(df, page_content_column="_combined_content")
            sheet_docs = loader.load()

            # ê° í–‰ì„ ìˆœíšŒí•˜ë©´ì„œ ë©”íƒ€ë°ì´í„° ë³´ê°•
            for idx, doc in enumerate(sheet_docs):
                row_data = df.iloc[idx]

                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
                doc.metadata.update(
                    {
                        "source": file_path,
                        "doc_type": "excel",
                        "sheet_name": sheet_name,
                        "row_number": idx + 2,
                        # ìš©ì–´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥ (ì •í™•í•œ ë§¤ì¹­ìš©)
                        "term": (
                            str(row_data[first_col])
                            if pd.notna(row_data[first_col])
                            else ""
                        ),
                        "definition": (
                            str(row_data[second_col])
                            if pd.notna(row_data[second_col])
                            else ""
                        ),
                    }
                )

                # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ (ì¹´í…Œê³ ë¦¬, íƒœê·¸ ë“±ì´ ìˆì„ ìˆ˜ ìˆìŒ)
                for col in df.columns:
                    if col not in [first_col, second_col, "_combined_content"]:
                        value = row_data[col]
                        if pd.notna(value):
                            if isinstance(value, (int, float)):
                                doc.metadata[col] = float(value)
                            else:
                                doc.metadata[col] = str(value)

            documents.extend(sheet_docs)

    return documents


# 5. êµ¬ì¡°í™”ëœ JSON ì²˜ë¦¬
def process_structured_json(file_path):
    """êµ¬ì¡°í™”ëœ JSONì„ DataFrameì²˜ëŸ¼ ì²˜ë¦¬"""
    import json
    import pandas as pd
    import numpy as np
    from langchain_community.document_loaders import DataFrameLoader

    def _is_notna_safe(v):
        res = pd.notna(v)
        if isinstance(res, (list, pd.Series, np.ndarray)):
            return bool(np.any(res))
        return bool(res)

    def _to_text(v):
        if isinstance(v, (list, dict)):
            return json.dumps(v, ensure_ascii=False)
        return str(v)

    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    documents = []

    # JSON ë°°ì—´ì„ DataFrameìœ¼ë¡œ ë³€í™˜
    if isinstance(data, list):
        # pandas json_normalizeë¡œ ì¤‘ì²© êµ¬ì¡° í‰íƒ„í™”
        df = pd.json_normalize(data)

        # ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ ë³€í™˜
        df["_combined_content"] = df.apply(
            lambda row: "\n".join(
                [
                    f"{col}: {_to_text(val)}"
                    for col, val in row.items()
                    if col != "_combined_content" and _is_notna_safe(val)
                ]
            ),
            axis=1,
        )

        # DataFrameLoader ì‚¬ìš©
        loader = DataFrameLoader(df, page_content_column="_combined_content")
        documents = loader.load()

        # ë©”íƒ€ë°ì´í„° ë³´ê°•
        for idx, doc in enumerate(documents):
            row_data = df.iloc[idx]

            doc.metadata.update(
                {
                    "source": file_path,
                    "doc_type": "json",
                    "item_index": idx,
                }
            )

            # ëª¨ë“  ì»¬ëŸ¼ì„ ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ê°€ (ê²€ìƒ‰/í•„í„°ë§ìš©)
            for col in df.columns:
                if col != "_combined_content":
                    value = row_data[col]
                    if _is_notna_safe(value):
                        # ë¦¬ìŠ¤íŠ¸ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜
                        if isinstance(value, list):
                            doc.metadata[col] = value
                        elif isinstance(value, (int, float)):
                            doc.metadata[col] = float(value)
                        else:
                            doc.metadata[col] = str(value)

    return documents


# 6. í†µí•© ë¬¸ì„œ ì²˜ë¦¬ ë° MongoDB ì €ì¥ í•¨ìˆ˜
def process_and_store_document(
    file_path, output_dir="output_images", move_after_process=True
):
    """
    ëª¨ë“  íƒ€ì…ì˜ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  MongoDBì— ì €ì¥
    """
    import shutil

    file_ext = Path(file_path).suffix.lower()
    print(f"\n{'='*60}")
    print(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘: {Path(file_path).name}")
    print(f"{'='*60}")

    try:
        # íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬
        if file_ext == ".pptx":
            documents = process_pptx(file_path, output_dir)
        elif file_ext in [".xlsx", ".xls"]:
            documents = process_excel(file_path)
        elif file_ext in [".txt", ".pdf", ".docx", ".doc"]:
            documents = process_text_document(file_path)
        elif file_ext == ".json":
            documents = process_structured_json(file_path)
        elif file_ext in [".goodocs"]:
            documents = process_goodocs(file_path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")

        print(f"ì¶”ì¶œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")

        # MongoDBì— ì €ì¥
        for idx, doc in enumerate(documents, 1):
            # ì„ë² ë”© ìƒì„± (ëŒ€/ì†Œë¬¸ì ë¬´ì‹œ ì •ê·œí™” ì ìš©)
            embedding_vector = text_embedder.embed_query(
                normalize_text(doc.page_content)
            )

            # MongoDB ë¬¸ì„œ êµ¬ì¡°
            mongo_doc = {
                "page_content": doc.page_content,
                "embedding": embedding_vector,
                "metadata": dict(doc.metadata),
            }

            # ì €ì¥
            result = collection.insert_one(mongo_doc)
            print(f"  âœ“ ë¬¸ì„œ {idx}/{len(documents)} ì €ì¥ ì™„ë£Œ")

        print(f"âœ… {Path(file_path).name} ì²˜ë¦¬ ì™„ë£Œ!\n")

        # ì²˜ë¦¬ ì™„ë£Œ í›„ íŒŒì¼ ì´ë™
        if move_after_process:
            # Complete_file í´ë” ê²½ë¡œ ì„¤ì •
            source_path = Path(file_path)
            complete_folder = source_path.parent.parent / "Complete_file"

            # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
            complete_folder.mkdir(parents=True, exist_ok=True)

            # ëª©ì ì§€ íŒŒì¼ ê²½ë¡œ
            destination_path = complete_folder / source_path.name

            # ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ì´ ì´ë¯¸ ìˆìœ¼ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            if destination_path.exists():
                from datetime import datetime

                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                stem = destination_path.stem
                suffix = destination_path.suffix
                destination_path = complete_folder / f"{stem}_{timestamp}{suffix}"

            # íŒŒì¼ ì´ë™
            shutil.move(str(source_path), str(destination_path))
            print(f"ğŸ“¦ íŒŒì¼ ì´ë™: {destination_path}")

        return len(documents)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


# %%


# %%
# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
    # collection.delete_many({})

    # ì—¬ëŸ¬ ë¬¸ì„œ ì²˜ë¦¬
    files_to_process = [
        # "/Users/daehwankim/Documents/langgraph-tutorial-main/RAG_CHATBOT/files/example.pptx",
        "/Users/daehwankim/Documents/langgraph-tutorial-main/RAG_CHATBOT/files/people.json",
        "/Users/daehwankim/Documents/langgraph-tutorial-main/RAG_CHATBOT/files/sample-word-document.docx",
        # "/Users/daehwankim/Documents/langgraph-tutorial-main/RAG_CHATBOT/files/titanic.xlsx",
        # "/path/to/your/document.pdf",
        # "/path/to/your/spreadsheet.xlsx",
        # "/path/to/your/document.docx",
    ]

    total_docs = 0
    for file_path in files_to_process:
        if os.path.exists(file_path):
            docs_count = process_and_store_document(
                file_path, move_after_process=True  # Falseë¡œ ì„¤ì •í•˜ë©´ ì´ë™ ì•ˆ í•¨
            )
            total_docs += docs_count
        else:
            print(f"âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    print(f"\n{'='*60}")
    print(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ! ì´ {total_docs}ê°œ ë¬¸ì„œê°€ MongoDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"{'='*60}")

# %%
