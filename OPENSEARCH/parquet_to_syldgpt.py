"""
Parquet íŒŒì¼ì„ syld_gpt OpenSearch ì¸ë±ìŠ¤ì— ì¶”ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Parquet íŒŒì¼ êµ¬ì¡°:
- ì»¬ëŸ¼ 0~4095: ì„ë² ë”© ê°’ (4096ì°¨ì›)
- ì»¬ëŸ¼ 4096: ì›ë˜ í…ìŠ¤íŠ¸ (page_content)

ì‚¬ìš©ë²•:
    python index_parquet_to_syld_gpt.py <parquet_file_path> [--batch-size BATCH_SIZE]

í™˜ê²½ ë³€ìˆ˜:
    OPENSEARCH_HOST: OpenSearch í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: localhost)
    OPENSEARCH_PORT: OpenSearch í¬íŠ¸ (ê¸°ë³¸ê°’: 9200)
    OPENSEARCH_USER: OpenSearch ì‚¬ìš©ìëª… (ê¸°ë³¸ê°’: admin)
    OPENSEARCH_PASSWORD: OpenSearch ë¹„ë°€ë²ˆí˜¸ (ê¸°ë³¸ê°’: admin)
    OPENSEARCH_USE_SSL: SSL ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: false)
"""

import os
import sys
import argparse
from pathlib import Path
from dotenv import load_dotenv
from opensearchpy import OpenSearch
import pandas as pd
import numpy as np
from tqdm import tqdm

load_dotenv(override=True)


def get_opensearch_client():
    """OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return OpenSearch(
        hosts=[
            {
                "host": os.getenv("OPENSEARCH_HOST", "localhost"),
                "port": int(os.getenv("OPENSEARCH_PORT", "9200")),
            }
        ],
        http_auth=(
            os.getenv("OPENSEARCH_USER", "admin"),
            os.getenv("OPENSEARCH_PASSWORD", "admin"),
        ),
        use_ssl=os.getenv("OPENSEARCH_USE_SSL", "false").lower() == "true",
        verify_certs=False,
        ssl_show_warn=False,
    )


def check_index_exists(client, index_name: str):
    """ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ ë° ì •ë³´ ì¶œë ¥"""
    if not client.indices.exists(index=index_name):
        print(f"âŒ ì˜¤ë¥˜: '{index_name}' ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        return False

    # ì¸ë±ìŠ¤ ë§¤í•‘ í™•ì¸
    mapping = client.indices.get_mapping(index=index_name)
    index_mapping = mapping[index_name]["mappings"]["properties"]

    print(f"âœ“ ì¸ë±ìŠ¤ '{index_name}' í™•ì¸ ì™„ë£Œ")

    # embedding ì°¨ì› í™•ì¸
    if "embedding" in index_mapping:
        embedding_dim = index_mapping["embedding"].get("dimension", "ì•Œ ìˆ˜ ì—†ìŒ")
        print(f"  - ì„ë² ë”© ì°¨ì›: {embedding_dim}")

    # ì¸ë±ìŠ¤ í†µê³„
    stats = client.indices.stats(index=index_name)
    total_docs = stats["indices"][index_name]["total"]["docs"]["count"]
    print(f"  - í˜„ì¬ ë¬¸ì„œ ìˆ˜: {total_docs:,}\n")

    return True


def load_parquet_file(file_path: str):
    """Parquet íŒŒì¼ ë¡œë“œ"""
    print(f"ğŸ“– Parquet íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
    df = pd.read_parquet(file_path)
    print(f"âœ“ ì´ {len(df):,}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ")
    return df


def extract_embedding_and_text(row, embedding_cols, text_col_idx):
    """
    í–‰ì—ì„œ ì„ë² ë”© ë²¡í„°ì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ

    Args:
        row: pandas Series (í–‰ ë°ì´í„°)
        embedding_cols: ì„ë² ë”© ì»¬ëŸ¼ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ (0~4095)
        text_col_idx: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì¸ë±ìŠ¤ (4096)

    Returns:
        tuple: (embedding_vector, page_content)
    """
    # ì„ë² ë”© ë²¡í„° ì¶”ì¶œ (ì»¬ëŸ¼ 0~4095)
    embedding_vector = row.iloc[embedding_cols].values.tolist()

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì»¬ëŸ¼ 4096)
    page_content = str(row.iloc[text_col_idx])

    return embedding_vector, page_content


def index_parquet_to_syld_gpt(
    parquet_file_path: str,
    index_name: str = "syld_gpt",
    batch_size: int = 1000,
    doc_type: str = "parquet",
    additional_metadata: dict = None,
):
    """
    Parquet íŒŒì¼ì„ syld_gpt OpenSearch ì¸ë±ìŠ¤ì— ì¶”ê°€

    Args:
        parquet_file_path: Parquet íŒŒì¼ ê²½ë¡œ
        index_name: OpenSearch ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: syld_gpt)
        batch_size: ë°°ì¹˜ í¬ê¸°
        doc_type: ë¬¸ì„œ íƒ€ì… (metadata.doc_typeì— ì €ì¥ë¨, ê¸°ë³¸ê°’: "parquet")
        additional_metadata: ì¶”ê°€í•  ë©”íƒ€ë°ì´í„° (ê° ë¬¸ì„œì— ê³µí†µìœ¼ë¡œ ì¶”ê°€ë¨)
    """
    # OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = get_opensearch_client()

    # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
    if not check_index_exists(client, index_name):
        return

    # Parquet íŒŒì¼ ë¡œë“œ
    df = load_parquet_file(parquet_file_path)

    # ì»¬ëŸ¼ í™•ì¸
    print(f"ğŸ“Š Parquet íŒŒì¼ ì»¬ëŸ¼ ì •ë³´:")
    print(f"   ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")

    # ì„ë² ë”© ì°¨ì› í™•ì¸ (ì»¬ëŸ¼ 0~4095 = 4096ê°œ)
    embedding_dim = 4096
    text_col_idx = 4096  # 4097ë²ˆì§¸ ì»¬ëŸ¼ (0-based indexë¡œëŠ” 4096)

    # ì»¬ëŸ¼ì´ ì¶©ë¶„í•œì§€ í™•ì¸
    if len(df.columns) < text_col_idx + 1:
        raise ValueError(
            f"Parquet íŒŒì¼ì— ì¶©ë¶„í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. "
            f"í•„ìš”: {text_col_idx + 1}ê°œ, ì‹¤ì œ: {len(df.columns)}ê°œ"
        )

    # ì„ë² ë”© ì»¬ëŸ¼ ì¸ë±ìŠ¤ (0~4095)
    embedding_cols = list(range(embedding_dim))

    # ë°°ì¹˜ë¡œ ì¸ë±ì‹±
    print(f"ğŸ“¤ OpenSearch ì¸ë±ìŠ¤ '{index_name}'ì— ì¸ë±ì‹± ì‹œì‘...")
    print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   doc_type: {doc_type}\n")

    total_rows = len(df)
    indexed_count = 0
    error_count = 0

    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
    default_metadata = {
        "doc_type": doc_type,  # doc_typeìœ¼ë¡œ êµ¬ë¶„
        "source": "parquet",
        "source_file": str(Path(parquet_file_path).name),
    }
    if additional_metadata:
        default_metadata.update(additional_metadata)

    # ë°°ì¹˜ ì²˜ë¦¬
    for start_idx in tqdm(
        range(0, total_rows, batch_size),
        desc="ì¸ë±ì‹± ì§„í–‰",
        total=(total_rows + batch_size - 1) // batch_size,
    ):
        end_idx = min(start_idx + batch_size, total_rows)
        batch_df = df.iloc[start_idx:end_idx]

        # Bulk APIìš© body ìƒì„±
        bulk_body = []

        for idx, (row_idx, row) in enumerate(batch_df.iterrows()):
            try:
                # ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                embedding_vector, page_content = extract_embedding_and_text(
                    row, embedding_cols, text_col_idx
                )

                # ì„ë² ë”© ë²¡í„° ê²€ì¦
                if len(embedding_vector) != embedding_dim:
                    print(
                        f"âš ï¸  ê²½ê³ : í–‰ {row_idx}ì˜ ì„ë² ë”© ì°¨ì›ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                        f"ì˜ˆìƒ: {embedding_dim}, ì‹¤ì œ: {len(embedding_vector)}"
                    )
                    error_count += 1
                    continue

                # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
                if not page_content or page_content.strip() == "":
                    print(f"âš ï¸  ê²½ê³ : í–‰ {row_idx}ì˜ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    error_count += 1
                    continue

                # OpenSearch ë¬¸ì„œ êµ¬ì¡°
                doc = {
                    "page_content": page_content,
                    "embedding": embedding_vector,
                    "metadata": default_metadata.copy(),
                }

                # Bulk API ì•¡ì…˜ ì¶”ê°€
                bulk_body.append({"index": {"_index": index_name}})
                bulk_body.append(doc)

            except Exception as e:
                print(f"âš ï¸  í–‰ {row_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                error_count += 1
                continue

        # Bulk APIë¡œ ì¸ë±ì‹±
        if bulk_body:
            try:
                response = client.bulk(body=bulk_body, refresh=False)

                # ê²°ê³¼ í™•ì¸
                for item in response.get("items", []):
                    if "index" in item:
                        if item["index"].get("status") in [200, 201]:
                            indexed_count += 1
                        else:
                            error_count += 1
                            if "error" in item["index"]:
                                error_info = item["index"].get("error", {})
                                print(
                                    f"âš ï¸  ì¸ë±ì‹± ì˜¤ë¥˜: {error_info.get('type', 'unknown')} - {error_info.get('reason', 'unknown')}"
                                )

            except Exception as e:
                print(f"âš ï¸  ë°°ì¹˜ ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                error_count += len(bulk_body) // 2

    # ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
    client.indices.refresh(index=index_name)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nâœ… ì¸ë±ì‹± ì™„ë£Œ!")
    print(f"   ì´ í–‰ ìˆ˜: {total_rows:,}")
    print(f"   ì„±ê³µ: {indexed_count:,}")
    print(f"   ì‹¤íŒ¨: {error_count:,}")

    # ì¸ë±ìŠ¤ í†µê³„ í™•ì¸
    stats = client.indices.stats(index=index_name)
    total_docs = stats["indices"][index_name]["total"]["docs"]["count"]
    print(f"   ì¸ë±ìŠ¤ ì´ ë¬¸ì„œ ìˆ˜: {total_docs:,}")

    # doc_typeë³„ í†µê³„ í™•ì¸
    try:
        search_body = {
            "size": 0,
            "aggs": {
                "doc_types": {"terms": {"field": "metadata.doc_type", "size": 20}}
            },
        }
        agg_response = client.search(index=index_name, body=search_body)
        doc_type_counts = {
            bucket["key"]: bucket["doc_count"]
            for bucket in agg_response["aggregations"]["doc_types"]["buckets"]
        }

        print(f"\nğŸ“Š doc_typeë³„ ë¬¸ì„œ ìˆ˜:")
        for doc_type_name, count in doc_type_counts.items():
            print(f"   - {doc_type_name}: {count:,}ê°œ")
    except Exception as e:
        print(f"âš ï¸  doc_type í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")

    client.close()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Parquet íŒŒì¼ì„ syld_gpt OpenSearch ì¸ë±ìŠ¤ì— ì¶”ê°€"
    )
    parser.add_argument(
        "parquet_file",
        type=str,
        help="ì¸ë±ì‹±í•  Parquet íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 1000)",
    )
    parser.add_argument(
        "--doc-type",
        type=str,
        default="parquet",
        help="ë¬¸ì„œ íƒ€ì… (metadata.doc_typeì— ì €ì¥ë¨, ê¸°ë³¸ê°’: parquet)",
    )

    args = parser.parse_args()

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    parquet_path = Path(args.parquet_file)
    if not parquet_path.exists():
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {parquet_path}")
        sys.exit(1)

    print("=" * 60)
    print("Parquet íŒŒì¼ â†’ syld_gpt ì¸ë±ìŠ¤ ì¶”ê°€")
    print("=" * 60)
    print(f"íŒŒì¼: {parquet_path}")
    print(f"ì¸ë±ìŠ¤: syld_gpt")
    print(f"doc_type: {args.doc_type}")
    print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}\n")

    # ì¸ë±ì‹± ì‹¤í–‰
    try:
        index_parquet_to_syld_gpt(
            parquet_file_path=str(parquet_path),
            index_name="syld_gpt",
            batch_size=args.batch_size,
            doc_type=args.doc_type,
        )
        print("\nâœ… ì‘ì—… ì™„ë£Œ!")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
