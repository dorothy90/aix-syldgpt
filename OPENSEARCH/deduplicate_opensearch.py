"""
OpenSearch ì¸ë±ìŠ¤ì—ì„œ page_content ê¸°ì¤€ ì¤‘ë³µ ì œê±° ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python deduplicate_opensearch.py

í™˜ê²½ ë³€ìˆ˜:
    OPENSEARCH_HOST: OpenSearch í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: localhost)
    OPENSEARCH_PORT: OpenSearch í¬íŠ¸ (ê¸°ë³¸ê°’: 9200)
    OPENSEARCH_USER: OpenSearch ì‚¬ìš©ìëª… (ê¸°ë³¸ê°’: admin)
    OPENSEARCH_PASSWORD: OpenSearch ë¹„ë°€ë²ˆí˜¸ (ê¸°ë³¸ê°’: admin)
    OPENSEARCH_USE_SSL: SSL ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: false)
"""

import os
import hashlib
from collections import defaultdict
from dotenv import load_dotenv
from opensearchpy import OpenSearch
from tqdm import tqdm

load_dotenv(override=True)


def get_opensearch_client():
    """OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return OpenSearch(
        hosts=[
            {
                "host": os.getenv("OPENSEARCH_HOST", "localhost"),
                "port": int(os.getenv("OPENSEARCH_PORT", "9200")),
            }
        ],
        http_auth=(
            os.getenv("OPENSEARCH_USER", "admin"),
            os.getenv("OPENSEARCH_PASSWORD", "admin"),
        ),
        use_ssl=os.getenv("OPENSEARCH_USE_SSL", "false").lower() == "true",
        verify_certs=False,
        ssl_show_warn=False,
    )


def scan_all_documents(client, index_name: str, batch_size: int = 1000):
    """
    OpenSearch ì¸ë±ìŠ¤ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ìŠ¤ìº”í•˜ì—¬ ë°˜í™˜

    Args:
        client: OpenSearch í´ë¼ì´ì–¸íŠ¸
        index_name: ì¸ë±ìŠ¤ ì´ë¦„
        batch_size: í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜

    Yields:
        ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬ (doc_id, page_content í¬í•¨)
    """
    # Scroll APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    search_body = {
        "size": batch_size,
        "_source": ["page_content", "embedding"],
    }

    response = client.search(
        index=index_name, body=search_body, scroll="5m"  # ìŠ¤í¬ë¡¤ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ ì‹œê°„
    )

    scroll_id = response.get("_scroll_id")
    hits = response["hits"]["hits"]

    # ì²« ë²ˆì§¸ ë°°ì¹˜ ì²˜ë¦¬
    for hit in hits:
        yield {
            "doc_id": hit["_id"],
            "page_content": hit["_source"].get("page_content", ""),
            "embedding": hit["_source"].get("embedding"),
        }

    # ë‚˜ë¨¸ì§€ ë¬¸ì„œë“¤ ìŠ¤í¬ë¡¤í•˜ì—¬ ê°€ì ¸ì˜¤ê¸°
    while len(hits) > 0:
        response = client.scroll(scroll_id=scroll_id, scroll="5m")
        scroll_id = response.get("_scroll_id")
        hits = response["hits"]["hits"]

        for hit in hits:
            yield {
                "doc_id": hit["_id"],
                "page_content": hit["_source"].get("page_content", ""),
                "embedding": hit["_source"].get("embedding"),
            }

    # ìŠ¤í¬ë¡¤ ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
    if scroll_id:
        client.clear_scroll(scroll_id=scroll_id)


def get_content_hash(page_content: str) -> str:
    """
    page_contentì˜ í•´ì‹œê°’ ê³„ì‚° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì¤‘ë³µ ê²€ì‚¬ìš©)

    SHA-256 í•´ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶©ëŒ ê°€ëŠ¥ì„±ì„ ê±°ì˜ ì—†ì•°
    """
    normalized_content = page_content.strip()
    return hashlib.sha256(normalized_content.encode("utf-8")).hexdigest()


def find_duplicates(client, index_name: str):
    """
    page_content ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ë¬¸ì„œ ì°¾ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë²„ì „)

    ì¤‘ìš”:
    - page_contentì˜ í•´ì‹œê°’ë§Œ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤
    - ëª¨ë“  ë¬¸ì„œë¥¼ í•œ ë²ˆì— ìŠ¤ìº”í•˜ì—¬ ë§¨ ì•ê³¼ ë§¨ ë’¤ ë¬¸ì„œì˜ ì¤‘ë³µë„ ì •í™•íˆ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - í•´ì‹œ ì¶©ëŒ ê°€ëŠ¥ì„±ì€ ê±°ì˜ ì—†ì§€ë§Œ, í•„ìš”ì‹œ ì‹¤ì œ ë‚´ìš© ë¹„êµë¡œ ê²€ì¦ ê°€ëŠ¥

    Returns:
        dict: {page_content_hash: [doc_ids]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    print(f"ğŸ“– '{index_name}' ì¸ë±ìŠ¤ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ìŠ¤ìº”í•˜ëŠ” ì¤‘...")
    print("   ğŸ’¡ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°©ì‹: page_contentì˜ í•´ì‹œê°’ë§Œ ì €ì¥í•©ë‹ˆë‹¤")
    print("   â†’ ë§¨ ì•ê³¼ ë§¨ ë’¤ ë¬¸ì„œì˜ ì¤‘ë³µë„ ì •í™•íˆ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

    # í•´ì‹œê°’ì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    # ì‹¤ì œ page_content ëŒ€ì‹  í•´ì‹œê°’ë§Œ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì„
    hash_to_docs = defaultdict(list)

    # ëª¨ë“  ë¬¸ì„œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìŠ¤ìº”
    doc_count = 0
    for doc in scan_all_documents(client, index_name):
        doc_count += 1
        page_content = doc["page_content"]

        # í•´ì‹œê°’ ê³„ì‚° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        content_hash = get_content_hash(page_content)
        hash_to_docs[content_hash].append(doc["doc_id"])

        # ì§„í–‰ ìƒí™© í‘œì‹œ (10000ê°œë§ˆë‹¤)
        if doc_count % 10000 == 0:
            print(
                f"   ì§„í–‰ ì¤‘... {doc_count:,}ê°œ ë¬¸ì„œ ìŠ¤ìº” ì™„ë£Œ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”)"
            )

    print(f"   âœ… ì´ {doc_count:,}ê°œ ë¬¸ì„œ ìŠ¤ìº” ì™„ë£Œ\n")

    # ì¤‘ë³µì´ ìˆëŠ” í•­ëª©ë§Œ í•„í„°ë§
    duplicates = {
        content_hash: doc_ids
        for content_hash, doc_ids in hash_to_docs.items()
        if len(doc_ids) > 1
    }

    print(f"   ğŸ“Š ê³ ìœ í•œ page_content í•´ì‹œ ê°œìˆ˜: {len(hash_to_docs):,}")
    print(f"   ğŸ“Š ì¤‘ë³µëœ page_content í•´ì‹œ ê°œìˆ˜: {len(duplicates):,}\n")

    return duplicates


def remove_duplicates(
    client,
    index_name: str,
    keep_strategy: str = "first",
    dry_run: bool = True,
):
    """
    ì¤‘ë³µ ë¬¸ì„œ ì œê±°

    Args:
        client: OpenSearch í´ë¼ì´ì–¸íŠ¸
        index_name: ì¸ë±ìŠ¤ ì´ë¦„
        keep_strategy: ìœ ì§€í•  ë¬¸ì„œ ì„ íƒ ì „ëµ
            - "first": ê° ê·¸ë£¹ì˜ ì²« ë²ˆì§¸ ë¬¸ì„œ ìœ ì§€ (ê¸°ë³¸ê°’)
            - "last": ê° ê·¸ë£¹ì˜ ë§ˆì§€ë§‰ ë¬¸ì„œ ìœ ì§€
        dry_run: Trueì´ë©´ ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰
    """
    # ì¤‘ë³µ ì°¾ê¸°
    duplicates = find_duplicates(client, index_name)

    if not duplicates:
        print("âœ… ì¤‘ë³µ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    total_duplicates = sum(len(doc_ids) - 1 for doc_ids in duplicates.values())
    total_unique_contents = len(duplicates)

    print(f"\nğŸ“Š ì¤‘ë³µ ë¶„ì„ ê²°ê³¼:")
    print(f"   - ì¤‘ë³µëœ page_content ê°œìˆ˜: {total_unique_contents:,}")
    print(f"   - ì‚­ì œë  ë¬¸ì„œ ê°œìˆ˜: {total_duplicates:,}")

    if dry_run:
        print("\nâš ï¸  DRY RUN ëª¨ë“œ: ì‹¤ì œë¡œ ì‚­ì œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("   ì‹¤ì œ ì‚­ì œë¥¼ ì›í•˜ë©´ dry_run=Falseë¡œ ì„¤ì •í•˜ì„¸ìš”.\n")
    else:
        print("\nğŸ—‘ï¸  ì‹¤ì œ ì‚­ì œ ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")

    # ì‚­ì œí•  ë¬¸ì„œ ID ìˆ˜ì§‘
    docs_to_delete = []

    for content, doc_ids in tqdm(
        duplicates.items(), desc="ì¤‘ë³µ ë¬¸ì„œ ì²˜ë¦¬", total=len(duplicates)
    ):
        if keep_strategy == "first":
            # ì²« ë²ˆì§¸ ë¬¸ì„œ ìœ ì§€, ë‚˜ë¨¸ì§€ ì‚­ì œ
            keep_id = doc_ids[0]
            delete_ids = doc_ids[1:]
        elif keep_strategy == "last":
            # ë§ˆì§€ë§‰ ë¬¸ì„œ ìœ ì§€, ë‚˜ë¨¸ì§€ ì‚­ì œ
            keep_id = doc_ids[-1]
            delete_ids = doc_ids[:-1]
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì „ëµ: {keep_strategy}")

        docs_to_delete.extend(delete_ids)

    if not docs_to_delete:
        print("âœ… ì‚­ì œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    # ì‹¤ì œ ì‚­ì œ ìˆ˜í–‰
    if not dry_run:
        print(f"\nğŸ—‘ï¸  {len(docs_to_delete):,}ê°œì˜ ì¤‘ë³µ ë¬¸ì„œë¥¼ ì‚­ì œí•˜ëŠ” ì¤‘...")

        # ë°°ì¹˜ë¡œ ì‚­ì œ (ì„±ëŠ¥ í–¥ìƒ)
        batch_size = 1000
        deleted_count = 0

        for i in tqdm(
            range(0, len(docs_to_delete), batch_size),
            desc="ë¬¸ì„œ ì‚­ì œ",
            total=(len(docs_to_delete) + batch_size - 1) // batch_size,
        ):
            batch = docs_to_delete[i : i + batch_size]

            # Bulk APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì‚­ì œ
            body = []
            for doc_id in batch:
                body.append({"delete": {"_index": index_name, "_id": doc_id}})

            response = client.bulk(body=body)

            # ì‚­ì œëœ ë¬¸ì„œ ìˆ˜ ì¹´ìš´íŠ¸
            for item in response.get("items", []):
                if "delete" in item and item["delete"].get("status") in [200, 404]:
                    deleted_count += 1

        print(f"\nâœ… ì™„ë£Œ! {deleted_count:,}ê°œì˜ ì¤‘ë³µ ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"\nğŸ“‹ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼:")
        print(f"   - ì‚­ì œë  ë¬¸ì„œ ID ê°œìˆ˜: {len(docs_to_delete):,}")
        print(f"   - ìœ ì§€ë  ë¬¸ì„œ ID ê°œìˆ˜: {total_unique_contents:,}")

        # ìƒ˜í”Œ ì¶œë ¥ (ì²˜ìŒ 5ê°œ)
        print("\nğŸ“ ì‚­ì œë  ë¬¸ì„œ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):")
        for i, doc_id in enumerate(docs_to_delete[:5]):
            print(f"   {i+1}. {doc_id}")
        if len(docs_to_delete) > 5:
            print(f"   ... ì™¸ {len(docs_to_delete) - 5}ê°œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    index_name = "syld_gpt"

    print("=" * 60)
    print("OpenSearch ì¤‘ë³µ ì œê±° ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    print(f"ì¸ë±ìŠ¤: {index_name}\n")

    # OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = get_opensearch_client()

    # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
    if not client.indices.exists(index=index_name):
        print(f"âŒ ì˜¤ë¥˜: '{index_name}' ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        return

    # ì¸ë±ìŠ¤ í†µê³„ í™•ì¸
    stats = client.indices.stats(index=index_name)
    total_docs = stats["indices"][index_name]["total"]["docs"]["count"]
    print(f"ğŸ“Š í˜„ì¬ ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜: {total_docs:,}\n")

    # ë¨¼ì € dry_runìœ¼ë¡œ í™•ì¸
    print("=" * 60)
    print("1ë‹¨ê³„: ì¤‘ë³µ ë¶„ì„ (DRY RUN)")
    print("=" * 60)
    remove_duplicates(
        client=client,
        index_name=index_name,
        keep_strategy="first",
        dry_run=True,
    )

    # ì‚¬ìš©ì í™•ì¸
    print("\n" + "=" * 60)
    response = input("ì‹¤ì œë¡œ ì¤‘ë³µ ë¬¸ì„œë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ").strip().lower()

    if response == "yes":
        print("\n" + "=" * 60)
        print("2ë‹¨ê³„: ì‹¤ì œ ì‚­ì œ ìˆ˜í–‰")
        print("=" * 60)
        remove_duplicates(
            client=client,
            index_name=index_name,
            keep_strategy="first",
            dry_run=False,
        )

        # ìµœì¢… í†µê³„
        stats_after = client.indices.stats(index=index_name)
        total_docs_after = stats_after["indices"][index_name]["total"]["docs"]["count"]
        print(f"\nğŸ“Š ì‚­ì œ í›„ ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜: {total_docs_after:,}")
        print(f"ğŸ“‰ ì‚­ì œëœ ë¬¸ì„œ ìˆ˜: {total_docs - total_docs_after:,}")
    else:
        print("\nâŒ ì‚­ì œ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    client.close()
    print("\nâœ… ì‘ì—… ì™„ë£Œ!")


if __name__ == "__main__":
    main()
